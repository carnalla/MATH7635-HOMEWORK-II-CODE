---
title: "MATH7635-HOMEWORK-II"
author: "Alexander M. Carnall"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

CODE CAN BE FOUND HERE: https://github.com/carnalla/MATH7635-HOMEWORK-II-CODE.git

## Chapter 4

### Question 4 - The Curse of Dimensionality (Page 189/190)

(a) Assume X is uniformly distributed on [0, 1]. Suppose we wish to predict a test observation's response using only observations within 10% of the range of X closest to the test observation. In order to predict the response for a test observation with X = 0.6, we will use observations in range [0.55, 0.65]. On average, what fraction of available observations will we use to make the prediction? 

Anywhere in 0.05 $\geq$ X $\leq$ 0.95, the fraction will be 10%. As we move to the left of 0.05, we lose 1% of observations for each 0.01 decrease in X. Similarly, as we move to the right of 0.95, we lose 1% of observations for each 0.01 increase in X. So for 90% of our distribution, we will get 10% of available observations. For 10% of our distribution, we will actually only get 7.5%. This amounts to: 

```{r}
(0.9 * 0.1) + (0.1 * 0.075)
```
An average fraction of observations equal to 9.75%.

(b) Now suppose we have a set of observations each with measurements on p = 2 features that are uniformly distributed on [0, 1] x [0, 1]. We wish to predict a test observation's response using only observations that are within 10% range of $X_1$ and within 10% of the range of $X_2$ closest to that test observation. On average, what fraction of the available observations will be use to make the prediction?

Since both variables are drawn from the same uniform distribution, they have equal average probabilities. The product of these values produces the fraction of available observations for this problem, i.e., $0.0975^p$ or $0.0975^2$ = 0.95%: 

```{r}
0.0975 * 0.0975
```

(c) Now suppose we have a set of observations on p = 100 features. Again, they are normally distributed on each feature, and again the features range from 0 to 1. What fraction of available observations will we use if they must each be within 10% of that feature's range for the test observation?

Using the same approach as above, $0.0975^p$ or $0.0975^{100}$, we get basically 0%: 

```{r}
0.0975^100
```

(d) Using answers from parts (a) - (c), argue that a drawback of KNN when p is large is that there are very few training observations 'near' any given test observation. 

I think the quantitative evidence for this is given already, but I think that this is a useful conceptual exercise actually. From a qualitative perspective, as more and more features/dimensions are added to the problem, each individual point becomes increasingly 'unique'. Eventually, this leads to a case where there are effectively zero observations that can be used to inform the classifier.

(e) Now suppose we wish to make a prediction for a test observation by creating a p-dimensional hypercube centered around the test observation that contains, on average, 10% of the training observations. For p = 1, 2, and 100, what is the length of each side of the hypercube?

Where the fixed value of 10% is in place for the volume of the cube, and a cube has equal side lengths which give volume when multiplied by one another (e.g., side length squared for a square, side length cubed for a cube, etc.). We can calculate side length in this case by side length = $p\sqrt{volume}$ so for p = 1; $1\sqrt{0.1} = 0.10$, for p = 2; $2\sqrt{0.1} = 0.316$, for p = 3; $3\sqrt{0.1} = 0.464$ for p = 100; $100\sqrt{0.1} = 0.977$

### Question 5 - Differences between LDA and QDA (Page 191)

(a) If the Bayes decision boundary is linear, we should expect that QDA will perform better on the training set since it can take on more flexible forms, including linear forms. This means it may capture some subtle nuances in the training data that may be missed by LDA. Conversely, where previously unseen test data is presented, we should expect that the inherent bias in LDA's linear form will outperform the likely overfit QDA model. 

(b) If the Bayes decision boundary is nonlinear, the flexibility offered by QDA should outperform LDA on both the training and test data. 

(c) With large sample size, the variance in the data decreases. In this case, QDA and it's associated flexibility should be able to model the data quite well with reduced risk of overfitting (due to large n, lower variance). In the case where n is smaller, this is a good case for LDA because the bias may make it more robust to overfitting. 

(d) FALSE. There is no guarantee that QDA will perform better than LDA on test data. While it is true that QDA has greater flexibility, if - for example - the Bayes decision boundary is actually linear, but the sample size is small (as described in part c above), QDA's flexibility may become a liability as it likely overfits the data and deviates from the true linear boundary described. 

### Question 6 - Multivariate Logistic Regression (Page 191)

(a) Estimate the probability that a student who studies for 40h and has UG GPA of 3.5 gets an 'A' in the class

$\hat{p}(X) = \displaystyle \frac{e^{\beta_0 + \beta_1X_1 + \beta_2X_2}}{1 + e^{\beta_0 + \beta_1X_1 + \beta_2X_2}}$ \
\
$\hat{p}(X) = \displaystyle \frac{e^{-6 + 0.05 \times 40 + 1 \times 3.5}}{1 + e^{-6 + 0.05 \times 40 + 1 \times 3.5}}$ \
\
$\hat{p}(X) = \displaystyle \frac{e^{-0.5}}{1 + e^{-0.5}}$ \
\
$\hat{p}(X) = \displaystyle \frac{0.606531}{1 + 0.606531}$ \
\
$\hat{p}(X) = 0.3775$ \
\
We can verify in R by:
``` {r}
exp(-6 + 0.05 * 40 + 1 * 3.5) / (1 + exp(-6 + 0.05 * 40 + 1 * 3.5))
```

The probability that this student receives an 'A' is 37.75%

(b) How many hours would student from 'a' need to study to have a 50% change of getting an 'A' in the class?

Given $\hat{p}(X) = 0.5$, and: \
\
$\hat{p}(X) = \displaystyle \frac{e^{\beta_0 + \beta_1X_1 + \beta_2X_2}}{1 + e^{\beta_0 + \beta_1X_1 + \beta_2X_2}}$ \
\
And \
\
$log(\frac{p(X)}{1 - p(X)}) = \beta_0 + \beta_1X_1 + \beta_2X_2$ \
\
Then: \
\
$log(\frac{0.5}{1 - 0.5}) = -6 + 0.05X_1 + 1 \times 3.5$ \
\
$log(1) = -2.5 + 0.05X_1$ \
\
$0 = -2.5 + 0.05X_1$ \
\
$2.5 = 0.05X_1$ \
\
$\frac{2.5}{0.05} = X_1$ \
\
$50 = X_1$ \
\
The student from part 'a' would need to study for 50 hours to have a 50% porbability of getting an 'A' in the class. We can verify by plugging '50' in for $X_1$ in the original equation in R, as:
```{r}
exp(-6 + 0.05 * 50 + 1 * 3.5) / (1 + exp(-6 + 0.05 * 50 + 1 * 3.5))
```

### Question 7 - Bayes Theorem/LDA; Predicting Probability of Dividends (Page 191)

In this question, it is stated that $X$ is normally distributed and that the variance of $X$ $(\hat\sigma^2)$ for these two sets of companies is equal, and we are given $p = 1$ so we can use:\
\
$pk(x) = \frac{\pi_k \frac{1}{\sqrt{2\pi\sigma}} exp(- \frac{1}{2\sigma^2}(x - \mu_k)^2)}{\sum_{l = 1}^K \pi{l} \frac{1}{\sqrt{2 \pi \sigma}} exp(-\frac{1}{2 \sigma^2}(x - \mu_l)^2)}$ \
\
In the problem text, we are told that $\overline{X} = 10$ for companies that did issue dividends, while $\overline{X} = 0$ for those that didn't, and that $\hat\sigma^2 = 36$. We are also told that 80% of all companies issued dividends, and we are supposed to predict the probability that a company for which $X = 4$ will issue a dividend. This means that: \
\
$\pi_{yes} = 0.80$ \
$\pi_{no} = 0.20$ \
$\overline{X}_{yes} = 10$ \
$\overline{X}_{no} = 0$ \
$\hat\sigma^2_{yes} = \hat\sigma^2_{no} = 36$ \
\
$p_{yes}(x) = \frac{0.80 \frac{1}{\sqrt{72\pi}} exp(- \frac{1}{2(36)}(-6)^2)}{0.80 \frac{1}{\sqrt{72\pi}} exp(-\frac{1}{2(36)}(-6)^2) + 0.20 \frac{1}{\sqrt{72\pi}} exp(-\frac{1}{2(36)}(4)^2)}$ \
\
$p_{yes}(x) = \frac{0.80 \frac{1}{\sqrt{72\pi}} exp(- \frac{1}{2(36)}(-6)^2)}{0.80 \frac{1}{\sqrt{72\pi}} exp(-\frac{1}{2(36)}(-6)^2) + 0.20 \frac{1}{\sqrt{72\pi}} exp(-\frac{1}{2(36)}(4)^2)}$ \
\
$p_{yes}(x) = \frac{0.80 \frac{1}{\sqrt{72\pi}} exp(- \frac{36}{72})}{0.80 \frac{1}{\sqrt{72\pi}} exp(-\frac{36}{72}) + 0.20 \frac{1}{\sqrt{72\pi}} exp(-\frac{16}{72})}$ \
\
$p_{yes}(x) = \frac{0.80 \frac{exp(-\frac{1}{2})}{\sqrt{72\pi}}}{0.80 \frac{exp(-\frac{1}{2})}{\sqrt{72\pi}} + 0.20 \frac{exp(-\frac{2}{9})}{\sqrt{72\pi}}}$ \
\
$p_{yes}(x) = \frac{0.80 \frac{0.60653}{15.03977}}{0.80 \frac{0.60653}{15.03977} + 0.20 \frac{0.80074}{15.03977}}$ \
\
$p_{yes}(x) = \frac{0.80 (0.04033)}{0.80 (0.04033) + 0.20 (0.05324)}$ \
\
$p_{yes}(x) = \frac{0.032264}{0.032264 + 0.010648}$ \
\
$p_{yes}(x) = \frac{0.032264}{0.042912}$ \

```{r}
0.032264 / 0.042912
```
The probability that this company with previous year percent profit 4 will issue a dividend is 75.19%.

### Question 8 - Training (20%) and Testing (30%) Error Using Logistic Regression vs. KNN with K = 1 (Average Error = 18%) (Page 191/192)

The question states explicitly that the error rate averaged over training and testing for KNN with K = 1 was 18%. Back in Chapter 3, we saw how when we use very small values of K - where K = 1 is the minimum - we greatly overfit the training data. This is illustrated in in figures 3.16 and 3.17 where KNN interpolates, or passes directly through, each of the training points. This means that for K = 1, the training error rate is actually zero. The average error rate of 18% over the testing and training data therefore implies then that the test error rate for KNN with K = 1 is actually 36% which is higher than that achieved using logistic regression (i.e., 30%). In summary, with respect to the question of which model we should prefer on classifying new observations, the answer seems obvious. Logistic regression will perform better in this case. 

### Question 9 - Odds Problems (Page 192)

(a) On average - what fraction of people with odds of 0.37 of defaulting on credit card payment will in fact default?

In equation 4.3, we are given that $\frac{p(X)}{1 - p(X)}$ is the odds, and that the average fitted probability describes the overall proportion of defaulters. Solving for this problem then involves setting:

$\frac{p(X)}{1 - p(X)} = 0.37$ \
\
$p(X) = 0.37(1 - p(X))$ \
\
$p(X) = 0.37 - 0.37p(X)$ \
\
$p(X) + 0.37p(X) = 0.37$ \
\
$1.37p(X) = 0.37$ \
\
$p(X) = \frac{0.37}{1.37}$ \

```{r}
0.37 / 1.37
```
The fraction of people with odds of 0.37 defaulting equals about 27.01%

(b) An individual has a 16% chance of defaulting on credit card payment, what are the odds she will default?

Again, we can leverage the equation from 4.3. This time we are converting probability to odds, as:

$\frac{p(X)}{1-p(X)} = odds$ \
\
$\frac{0.16}{1 - 0.16} = odds$ \
\
$\frac{0.16}{0.84} = odds$ \

```{r}
0.16 / 0.84
```
An individual with a 16% probability of defaulting has odds of 0.19 that she will actually default. 

### Question 12 - Log Odds, Logistic Regression (Page 192/193)

You fit a logistic regression model using: 

$\hat{Pr}(Y = orange | X = x) = \frac{exp(\hat{\beta_0} + \hat{\beta_1}x)}{1 + exp(\hat{\beta_0} + \hat{\beta_1}x)}$ \
\
Your friend fits a logistic regression model to the same data using softmax formulation from (4.13):

$\hat{Pr}(Y = orange | X = x) = \frac{exp(\hat{\alpha}_{orange0} + \hat{\alpha}_{orange1}x)}{exp(\hat{\alpha}_{orange0} + \hat{\alpha}_{orange1}x) + exp(\hat{\alpha}_{apple0} + \hat{\alpha}_{apple1}x)}$ \
\

(a) What is the log odds of orange vs. apple in your model?

In equation 4.2, it is shown that the log odds between any pair of classes is linear in the features. This means, for our case: 

$logodds(\hat{Pr}(Y = orange | X = x)) = \hat{\beta}_0 + \hat{\beta}_{1x}$ \
\

(b) What is the log odds of orange vs. apple in your friend's model?

In equation 4.14, it is shown that the log odds ratio in the case of our friend's softmax model approach becomes: 

$logodds(\frac{\hat{Pr}(Y = orange | X = x)}{\hat{Pr}(Y = apple | X = x)}) = (\hat{\alpha}_{orange0} - \hat{\alpha}_{apple0}) + (\hat{\alpha}_{orange1} - \hat{\alpha}_{apple1}) x$ \
\

(c) Suppose that in your model, $\hat{\beta_0} = 2$, and $\hat{\beta_1} = -1$. What are the coefficient estimates in your friend's model? Be as specific as possible.

$\hat{\beta}_0 = 2 = (\hat{\alpha}_{orange0} - \hat{\alpha}_{apple0})$ \
\
$\hat{\beta}_1 = -1 = (\hat{\alpha}_{orange1} - \hat{\alpha}_{apple1})$ \
\

(d) Now suppose you and your friend fit the same two models on a different data set. This time your friend gets the coefficient estimates $\hat{\alpha}_{orange0} = 1.2$, $\hat{\alpha}_{orange1} = -2$, $\hat{\alpha}_{apple0} = 3$, and $\hat{\alpha}_{apple1} = 0.6$ What are the coefficient estimates in your model?

$\hat{\beta}_0 = (1.2 - 3) = -1.8$ \
\
$\hat{\beta}_1 = (-2 - 0.6) = -2.6$ \
\

(e) Finally, suppose you apply both models from (d) to a data set with 2,000 test observations. What fraction of the time do you expect the predicted label class from your model to agree with those from your friend's model? Explain your answer.

On page 141, it is acknowledged that the fitted values, log odds between any pair of classes, and other key model outputs will remain the same, regardless of coding. The main difference is only in how the groups are coded relative to one another. I expect that the predicted class labels from these two models will be identical. 


### Question 13 - Weekly Data Set (Page 193)

(a) Produce some numerical and graphical summaries of the Weekly data. Do there appear to be any patterns?

```{r}
library(ISLR2)
?Weekly
summary(Weekly)
pairs(Weekly)
colors = c('red', 'green')[unclass(Weekly$Direction)]
pairs(Weekly, col = colors)
```

After viewing the black and white scatter plots, it occurred to me that I might be missing some detail in the response variable (Direction), so I added some color to the plots based on the factor level in this response. At this point, it seems clear to me that Year and Volume share a quadratic relationship where the volume of daily shares traded (in billions) has increased over the years. Next, it might be worth adding a dummy-coded variable to a copy of the dataset to confirm whether or not I might be missing anything else. 

```{r}
Weekly2 = Weekly
Weekly2$Direction2 = ifelse(Weekly$Direction == 'Up', 1, 0)
round(cor(Weekly2[, c(1:8, 10)]), 2)
```
It seems that the relationship between Year and Volume can be confirmed with a correlation of r = 0.84. It seems now as if there is also a positive association (r = 0.72) between 'Today' and 'Direction' whereby high percentage returns for this week are related to an overall positive return for the week. This is not surprising, but was not spelled out explicitly until now. 

(b) Use the full data set to perform logistic regression with Direction as the response and the five lag variables plus volume as predictors. Use summary() function to print results. Do any of the predictors appear to be statistically significant? If so, which ones?

```{r}
weeklyLR = glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Weekly, 
               family = binomial)
summary(weeklyLR)
```

After fitting the logistic regression model using the glm() function, it appears as if the only significant predictor variable was Lag2 (percentage return for two weeks previous) with coefficient and p-value of: $\hat{\beta}_2 = 0.058, p = 0.0296$. 

(c) Compute the confusion matrix and overall fraction of correct predictions. Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.

I first make a call to contrasts to make sure that the correct values are used in the 'directionPreds' vector. The confusion matrix is computed afterward by tabling the predictions for 'Up' if the probabilities are returned greater than 0.5 vs. the original 'Direction' variable from the Weekly data. 

```{r}
contrasts(Weekly$Direction)
weeklyPreds = predict(weeklyLR, type = 'response')
directionPreds = rep('Down', nrow(Weekly))
directionPreds[weeklyPreds > 0.5] = 'Up'
conMat = table(directionPreds, Weekly$Direction)
conMat
```
In this table, the row values are the predicted direction of the market, and the column values are the true class labels provided in the original data. The prediction accuracy of the training data can be computed by summing the diagonal elements and dividing this total by the sum of all table elements. The calculation for this is shown next: 

```{r}
correct = sum(diag(conMat)) / sum(conMat)
correct
```
The percentage of correct predictions is ~56%. By comparison, the error rate could be computed as 1 - this value or by summing the off-diagonal elements and dividing by the sum of all table elements. It is easy to see in this case that the prediction error rate would be about 44%. 

Concerning the specific types of errors being made by the logistic regression model, we can also compute the Type I and Type II error rates as shown on page 152. Type I errors are 'false positives', and in this case can be given by the fraction of 'Up' predictions among all true 'Down' labels (i.e., FP / N). This is given by: 

```{r}
conMat[2, 1] / sum(conMat[, 1])
```
This is quite a large false positive rate (about 89%). Conversely, we can compute the Type II error rate which equates to the fraction of predicted 'Down' cases among all true 'Up' cases (i.e., false negatives; FN / P) by:

```{r}
conMat[1, 2] / sum(conMat[, 2])
```
This shows a false negative rate of about 8%. We can also compute the sensitivity (or true positive rate) as the fraction of predicted 'Up' cases among all true 'Up' cases, or the specificity (true negative rate) as the fraction of predicted 'Down' cases among all true 'Down' cases as:

```{r}
#sensitivity:
conMat[2, 2] / sum(conMat[, 2])

#specificity:
conMat[1, 1] / sum(conMat[, 1])
```
These results show a sensitivity of about 92%, and a specificity of about 11%. In addition, we would be able to describe the position on the ROC curve for the set threshold of 0.5 in our logistic regression model using the false positive rate calculated earlier and the true positive (sensitivity) rate just calculated giving X, Y coordinates of (0.89, 0.92), which is quite poor. This actually agrees with the overall accuracy of ~56% when considering the location of this coordinate on the ROC plot. It would very closely approximate the upper right-hand corner of the plot, and this is where we expect that point to be for a 'no information' classifier. Not very good! Collectively, the results seem to suggest that our current classifier is not very good at accurately discriminating between classes.

(d) Now fit the logistic regression model using a training data period from 1990 - 2008, with Lag2 as the only predictor. Compute the confusion matrix and the overall fraction of correct predictions for the held out data (all data from 2009-10).

```{r}
weeklyTrain = Weekly[Weekly$Year  < 2009, ]
weeklyTest = Weekly[Weekly$Year >= 2009, ]

weeklyLRModel = glm(Direction ~ Lag2, data = weeklyTrain, family = 'binomial')
weeklyLRPreds = predict(weeklyLRModel, weeklyTest, type = 'response')
weeklyLRResps = rep('Down', nrow(weeklyTest))
weeklyLRResps[weeklyLRPreds > 0.5] = 'Up'
weeklyLRTable = table(weeklyLRResps, weeklyTest$Direction)
weeklyLRTable
```
The overall prediction accuracy of the logistic regression model on the test data was 62.5% and can be summarized by: 

```{r}
LRAccuracy = sum(diag(weeklyLRTable)) / sum(weeklyLRTable)
LRAccuracy
```

(e) Repeat (d) using LDA

```{r}
library(MASS)
weeklyLDAModel = lda(Direction ~ Lag2, data = weeklyTrain)
weeklyLDAPreds = predict(weeklyLDAModel, weeklyTest, type = 'response')
weeklyLDATable = table(weeklyLDAPreds$class, weeklyTest$Direction)
weeklyLDATable
```
The overall prediction accuracy of LDA is identical to that of logistic regression at 62.5%, and can be summarized by:

```{r}
LDAAccuracy = sum(diag(weeklyLDATable)) / sum(weeklyLDATable)
LDAAccuracy
```

(f) Repeat (d) using QDA

```{r}
weeklyQDAModel = qda(Direction ~ Lag2, data = weeklyTrain)
weeklyQDAPreds = predict(weeklyQDAModel, weeklyTest, type = 'response')
weeklyQDATable = table(weeklyQDAPreds$class, weeklyTest$Direction)
weeklyQDATable
```
The overall prediction accuracy of QDA is lower than the previous two models, at 58.6%, and can be summarized by:

```{r}
QDAAccuracy = sum(diag(weeklyQDATable)) / sum(weeklyQDATable)
QDAAccuracy
```

(g) Repeat (d) using KNN with K = 1

```{r}
#We have seen 'caret' and 'class' packages used for KNN in this course, 
#I have opted to go with 'class' in this question based on the textbook (4.7.6)
library(class)

weeklyKNNtrain = cbind(weeklyTrain$Lag2)
weeklyKNNtest = cbind(weeklyTest$Lag2)
weeklyTrainLabels = weeklyTrain$Direction

set.seed(1)

weeklyKNNPred = knn(weeklyKNNtrain, weeklyKNNtest, weeklyTrainLabels, k = 1)
weeklyKNNTable = table(weeklyKNNPred, weeklyTest$Direction)
weeklyKNNTable

```
The overall prediction accuracy of KNN with K = 1 is the worst of all so far with exactly 50% accuracy - the same that would be expected with a 'no information' approach: 

```{r}
KNNAccuracy = sum(diag(weeklyKNNTable)) / sum(weeklyKNNTable)
KNNAccuracy
```

(h) Repeat (d) using naive Bayes

```{r}
library(e1071)
weeklyNBModel = naiveBayes(Direction ~ Lag2, data = weeklyTrain)
weeklyNBPreds = predict(weeklyNBModel, weeklyTest)
weeklyNBTable = table(weeklyNBPreds, weeklyTest$Direction)
weeklyNBTable
```
The overall prediction accuracy for naive Bayes in this case is about 59%, shown below: 

```{r}
NBAccuracy = sum(diag(weeklyNBTable)) / sum(weeklyNBTable)
NBAccuracy
```

(i) Which of these methods appears to provide the best results on this data?

```{r}
summary = data.frame(rbind(LRAccuracy, LDAAccuracy, QDAAccuracy, KNNAccuracy, NBAccuracy))
names(summary) = 'Accuracy'
summary
```
Cumulatively, it appears as if the best performance on this data was achieved by the Logistic Regression ('LRAccuracy' in the above dataframe) and Linear Discriminant Analysis ('LDAAccuracy' in the above dataframe). Each of these methods achieved equivalent overall prediction accuracy of 62.5%. QDA and naive Bayes both achieved overall prediction accuracies of ~59% but in both cases, did not predict any negative (i.e., 'Down') class labels. 

(j) Experiment with different combinations of predictors, including possible transformations and interactions, for each of the methods. Report the variables, method, and associated confusion matrix that appears to provide the best results on the held-out data. Note that you should also experiment with values for K in the KNN classifier. 

Thinking that I might be able to uncover some previously less obvious relationships, I tried generating a more descriptive set of plots for the Weekly data...

```{r, message = FALSE}
library(ggplot2)
library(GGally)

ggpairs(Weekly, ggplot2::aes(color = Direction))
```
It is looking increasingly like the Direction target variable will be very difficult to predict accurately using the Year, Lag, and Volume variables. Obviously, we knew that only one variable had a significant association with 'Direction' earlier, and the 'Today' variable is coupled to 'Direction' making it a non-starter as it will not be known a priori. Nevertheless, we proceed with the exercise, and I will use the same training and testing sets as in the previous parts of this problem. 

```{r}
#Logistic Regression
#For this first model, I will use an interaction between Lag2 and Year as a predictor variable. 

wklyLR2Mdl = glm(Direction ~ Lag2:Year, data = weeklyTrain, family = 'binomial')
wklyLR2Prd = predict(wklyLR2Mdl, weeklyTest, type = 'response')
wklyLR2Rsp = rep('Down', nrow(weeklyTest))
wklyLR2Rsp[wklyLR2Prd > 0.5] = 'Up'
wklyLR2Tbl = table(wklyLR2Rsp, weeklyTest$Direction)
wklyLR2Tbl
sum(diag(wklyLR2Tbl)) / sum(wklyLR2Tbl)
```
Not a very good start, the prediction accuracy for this model is the same as before. 


```{r}
#Linear Discriminant Analysis
#For this model, I will use the mean of all lag variables as a predictor variable.
#Since a new variable is created, I will need to re-declare training and test sets.

Weekly$MeanLag = rowMeans(Weekly[, c(2:6)])
weeklyTrain = Weekly[Weekly$Year  < 2009, ]
weeklyTest = Weekly[Weekly$Year >= 2009, ]

wklyLDA2Mdl = lda(Direction ~ MeanLag, data = weeklyTrain)
wklyLDA2Prd = predict(wklyLDA2Mdl, weeklyTest, type = 'response')
wklyLDA2Tbl = table(wklyLDA2Prd$class, weeklyTest$Direction)
wklyLDA2Tbl
sum(diag(wklyLDA2Tbl)) / sum(wklyLDA2Tbl)
```
This LDA model performed slightly worse than the original LDA model in this problem. 

```{r}
#Quadratic Discriminant Analysis
#Perhaps the mean of only the most recent two weeks will be better...

Weekly$MeanLag2 = rowMeans(Weekly[, c(2:3)])
weeklyTrain = Weekly[Weekly$Year  < 2009, ]
weeklyTest = Weekly[Weekly$Year >= 2009, ]

wklyQDA2Mdl = qda(Direction ~ MeanLag2, data = weeklyTrain)
wklyQDA2Prd = predict(wklyQDA2Mdl, weeklyTest, type = 'reponse')
wklyQDA2Tbl = table(wklyQDA2Prd$class, weeklyTest$Direction)
wklyQDA2Tbl
sum(diag(wklyQDA2Tbl)) / sum(wklyQDA2Tbl)
```
This QDA model performed identically to the previous version of QDA in this problem.

```{r}
#naive Bayes
#For this model I will use a combination of MeanLag2 and Year 

wklyNB2Mdl = naiveBayes(Direction ~ MeanLag2 + Year, data = weeklyTrain)
wklyNB2Prd = predict(wklyNB2Mdl, weeklyTest)
wklyNB2Tbl = table(wklyNB2Prd, weeklyTest$Direction)
wklyNB2Tbl
sum(diag(wklyNB2Tbl)) / sum(wklyNB2Tbl)

```
This is the worst model yet, accurately classifying only ~41% of test cases... Worse than a 'no information' classifier.

```{r}
#KNN
#Given the results to this point, and the prediction accuracy (50%) 
#of KNN in the previous part of the problem,
#I will again use Lag2 as the predictor variable, but experiment with manipulating 
#the value of K used. 

maxK = 50

wklyKNN2Results = data.frame(matrix(ncol = 2, nrow = maxK))
names(wklyKNN2Results) = c('K', 'Accuracy')
wklyKNN2Tbls = list()

set.seed(1)

for (NN in seq(1:maxK)) {
  
  wklyKNN2Pred = knn(weeklyKNNtrain, weeklyKNNtest, weeklyTrainLabels, k = NN)
  wklyKNN2Tbl = table(wklyKNN2Pred, weeklyTest$Direction)
  accuracy = sum(diag(wklyKNN2Tbl)) / sum(wklyKNN2Tbl)
  wklyKNN2Results$K[NN] = NN
  wklyKNN2Results$Accuracy[NN] = accuracy
  wklyKNN2Tbls[[NN]] = wklyKNN2Tbl
  
}

KNNplot = plot(wklyKNN2Results$K, wklyKNN2Results$Accuracy,
               main = 'KNN Prediction Accuracy for Direction on Lag2',
               xlab = 'Value for K',
               ylab = 'Prediction Accuracy (%)',
               type = 'o')

KNNplot
```
```{r}

print(paste('The highest prediction accuracy was for K = ', 
            wklyKNN2Results$K[wklyKNN2Results$Accuracy == max(wklyKNN2Results$Accuracy)], 
            ' with an accuracy score of ', 
            round(wklyKNN2Results$Accuracy[wklyKNN2Results$Accuracy == max(wklyKNN2Results$Accuracy)], 2), 
            sep = ''))
```

The best prediction accuracy in this problem (somewhat ironically) was the logistic regression model using the 'Lag2' and 'Year' interaction variable. It achieved an accuracy of 62.5% which is no better than the prior logistic regression model in this same problem. Cumulatively, this was not an easy problem, and maybe reflects real-world difficulty associated with accurately forecasting changes in the stock market.

### Question 14 - In this problem, you will develop a model to predict whether a given car gets high or low gas mileage based on the Auto dataset (Page 194). 

(a) Create a binary variable 'mpg01' that contains a 1 if the associated mpg value is above median, and a zero if it is below. 

```{r}
?Auto
head(Auto)

#store median MPG value in a new variable, and check if any existing mpg values are equal to it... 
#If not, we can use strictly the '>' operator to distinguish above/below median.
medMPG = median(Auto$mpg)
sum(Auto[Auto$mpg == medMPG])

mpg01 = ifelse(Auto$mpg > medMPG, 1, 0)
Auto2 = cbind(Auto, mpg01)
head(Auto2)
```

(b) Explore the data graphically in order to investigate the association between mpg01 and the other features. Which of the other features seem most likely to be useful in predicting mpg01? Scatterplots and boxplots may be useful tools to answer this question. Describe your findings.

```{r}
autoColors = c('blue', 'orange')[factor(Auto2$mpg01)]
pairs(Auto2[, c(2:8, 10)], col = autoColors)

```
```{r}
par(mfrow = c(2, 3))
boxplot(cylinders ~ mpg01, data = Auto2)
boxplot(displacement ~ mpg01, data = Auto2)
boxplot(horsepower ~ mpg01, data = Auto2)
boxplot(weight ~ mpg01, data = Auto2)
boxplot(acceleration ~ mpg01, data = Auto2)
boxplot(year ~ mpg01, data = Auto2)
```

The pairs plots with binary color-coded response variable were helpful in visually evaluating which variables may have value in predicting those vehicles with greater or less than median fuel economy (i.e., mpg). Subsequently, the boxplots appear to provide a more precise description of which variables are most likely to discriminate between classes of the response. It seems  likely that most of these variables have the capacity to explain a high fraction of variance in the response (with the possible exception of acceleration), but I am concerned that several may be collinear (i.e., those which have the same pattern of response - low for the zero class and high for the one class as other variables). It is also probable that some of these predictors are mechanistically related to one another (e.g., cylinders, displacement, and horsepower). 

```{r}
par(mfrow = c(2, 2))
plot(Auto2$cylinders, Auto2$displacement, col = autoColors)
plot(Auto2$horsepower, Auto2$displacement, col = autoColors)
plot(Auto2$horsepower, Auto2$cylinders, col = autoColors)

```
With the understanding that these variables each seem to share some association with each other as well as the response variable, I wanted next to see which appear most strongly related to the response variable in it's continuous form. A red horizontal line is included in each plot to show where the median value lies and aid in separation between classes. 

```{r}
par(mfrow = c(2, 2))
plot(Auto2$cylinders, Auto2$mpg, col = autoColors)
abline(h = medMPG, col = 'red')
plot(Auto2$horsepower, Auto2$mpg, col = autoColors)
abline(h = medMPG, col = 'red')
plot(Auto2$displacement, Auto2$mpg, col = autoColors)
abline(h = medMPG, col = 'red')
```

At this point, I am going to make a judgement call that of these three variables (which are related to one another, AND each independently related - to varying degrees - with the response), the most robust predictor will be displacement given that along the x-axis (displacement), there seems to be greater discrimination between the below, and above median response, while for the other two (horsepower, and cylinders) there seems to be a high number of cases in each class given any fixed value of the predictor. Logically, vehicle weight has a good deal of value in predicting fuel economy since a lighter vehicle should have a lower energetic cost of transport, and is not associated with model year. For the models in subsequent parts, I will evaluate the ability of displacement, vehicle weight, and model year to predict binary response 'mpg01'. 

(c) In the absence of any predefined convention for partitioning the data into training and test sets, I will deviate slightly from the earlier 'Weekly' analysis because this is a substantially smaller dataset and use an 80/20 split for this exercise. The first step will be selecting only the predictors I have committed to, and then I will perform the partition.

```{r}
set.seed(1)
AutoNew = Auto2[, c(3, 5, 7, 10)]
trainInd = sample(nrow(AutoNew), 0.80 * nrow(AutoNew), replace = FALSE)

autoTrain = AutoNew[trainInd, ]
autoTest = AutoNew[-trainInd, ]
```

(d) Perform LDA on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in (b). What is the test error of the model obtained?

```{r}
autoLDAModel = lda(mpg01 ~ ., data = autoTrain)
autoLDAPreds = predict(autoLDAModel, autoTest, type = 'response')
autoLDATable = table(autoLDAPreds$class, autoTest$mpg01)
autoLDATable
```
In the table shown in this output, the initial results are encouraging! Test error can be given by:

```{r}
1 - sum(diag(autoLDATable)) / sum(autoLDATable)
```
The LDA model produced a test error rate of 10%, not a bad start. 

(e) Paraphrased: repeat (d) using QDA. 

```{r}
autoQDAModel = qda(mpg01 ~ ., data = autoTrain)
autoQDAPreds = predict(autoQDAModel, autoTest, type = 'response')
autoQDATable = table(autoQDAPreds$class, autoTest$mpg01)
autoQDATable
```
In the table from the QDA output, these results seem even better, sum of off-diagonal elements is only 6 compared to 8 in the previous model. Test error can be given by:

```{r}
1 - sum(diag(autoQDATable)) / sum(autoQDATable)
```
The QDA model produced a test error rate of about 7.6%. Even better. 

(f) Paraphrased: repeat (d) using Logistic Regression.

```{r}
autoLRModel = glm(mpg01 ~ ., data = autoTrain, family = 'binomial')
autoLRPreds = predict(autoLRModel, autoTest, type = 'response')
autoLRLabel = rep('0', nrow(autoTest))
autoLRLabel[autoLRPreds > 0.5] = '1'
autoLRTable = table(autoLRLabel, autoTest$mpg01)
autoLRTable
```
The Logistic Regression model has the same sum of off-diagonal elements as the QDA model, test error rate will be the same. Given anyway by:

```{r}
1 - sum(diag(autoLRTable)) / sum(autoLRTable)
```

(g) Paraphrased: repeat (d) using naive Bayes

```{r}
autoNBModel = naiveBayes(mpg01 ~ ., data = autoTrain)
autoNBPreds = predict(autoNBModel, autoTest)
autoNBTable = table(autoNBPreds, autoTest$mpg01)
autoNBTable
```
The naive Bayes classifier has a test error rate of about 8.8%, slightly better than the 10% prediction error rate of LDA.

```{r}
1 - sum(diag(autoNBTable)) / sum(autoNBTable)
```

(h) Paraphrased: repeat (d) using KNN, but with several values of K and identify the best value of K (will be based on test error rate)

```{r}
autoKNNTrain = scale(autoTrain[, c(1:3)])
autoKNNTest = scale(autoTest[, c(1:3)])
autoKNNLabels = autoTrain$mpg01

set.seed(1)

numKvalues = 20
knnOutputs = data.frame(matrix(ncol = 2, nrow = numKvalues))
names(knnOutputs) = c('K', 'Error')
knnTables = list()

for (knum in seq(1:numKvalues)) {
  autoKNNPreds = knn(autoKNNTrain, autoKNNTest, autoKNNLabels, k = knum)
  autoKNNTable = table(autoKNNPreds, autoTest$mpg01)
  knnTables[[knum]] = autoKNNTable
  knnErrorRate = 1 - sum(diag(autoKNNTable)) / sum(autoKNNTable)
  knnOutputs$K[knum] = knum
  knnOutputs$Error[knum] = knnErrorRate
}

plot(knnOutputs$K, knnOutputs$Error, type = 'b', col = 'blue', 
     main = 'Error Rate for KNN Classifier by Value of K', 
     xlab = 'Value for K', 
     ylab = 'Test Error Rate', 
     ylim = c(0, max(knnOutputs$Error) + 0.05))
```

This output suggests that the optimal value for K (based only on test error rate) might be K = 4 or K = 5 for this problem. The resultant test error rate for these classifiers was 5% Their respective confusion matrices are output below:

```{r}
#Confusion matrix for k = 4:
knnTables[[4]]
#Confusion matrix for k = 5:
knnTables[[5]]
```

### Question 15 - This problem involves writing functions (Page 194/195)

(a) Write a function, Power(), that prints out the result of raising 2 to the 3rd power. Your function should compute $2^3$ and print out the results.

```{r}
Power = function(x) {
  output = x ^ 3
  print(output)
}

Power(2)
```

(b) Create a new function, Power2(), that allows you to pass any two numbers, x and a, and prints out the value of $x^a$.

```{r}
Power2 = function(x, a) {
  output2 = x^a
  print(output2)
}

Power2(3, 8)
```

(c) Using the function you just wrote, compute $10^3$, $8^17$, and $131^3$

```{r}
Power2(10, 3)
Power2(8, 17)
Power2(131, 3)
```

(d) Now create a new function, Power3(), that actually returns the result $x^a$ as an R object, rather than simply printing it to the screen. If you store the value $x^a$ in an object called result within your function, then you can simple return() this result using the following line. 

```{r}
Power3 = function(x, a) {
  result = x^a
  return(result)
}

result = Power3(2, 3)
result
```

(e) Now using the Power3() function, create a plot of $f(x) = x^2$. The x-axis should display a range of integers from 1 to 10, and the y-axis should display $x^2$. Label the axes appropriately, and use an appropriate title for the figure. Consider displaying either the x-axis, y-axis, or both on the log-scale. This can be done using log = 'x', log = 'y', or log = 'xy' as arguments to the plot() function. 

```{r}
x = (1:10)
y = Power3(x, 2)
par(mfrow = c(2, 2))
plot(x, y, main = 'Plot of Y = x²', xlab = 'x', ylab = 'x²')
plot(x, y, main = 'Plot of Y = x² with log-scale X-axis', xlab = 'log(x)', ylab = 'x²', log = 'x')
plot(x, y, main = 'Plot of Y = x² with log-scale Y-axis', xlab = 'x', ylab = 'log(x²)', log = 'y')
plot(x, y, main = 'Plot of Y = x² with log-scale X- and Y-axis', 
     xlab = 'log(x)', ylab = 'log(x²)', log = 'xy')
```

(f) Create a function, PlotPower(), that allows you to create a plot of x against $x^a$ for a fixed a, and a range of values for x. For instance if you call PlotPower(1:10, 3), then a plot should be created with an x-axis taking on values 1, 2,..., 10, and a y-axis taking on values $1^3, 2^3,...,10^3$

```{r}
PlotPower = function(x, a){
  y = x^a
  plot(x, y, main = paste('Plot of Y = x^', a, '  Based on User Input', sep = ''), 
       xlab = 'x-values', ylab = paste('x^', a, sep = ''))
}

PlotPower(1:10, 3)
```

### Question 16: Using the Boston data set, fit classification models in order to predict whether a given census tract has a crime rate above or below the median. Explore Logistic Regression, LDA, naive Bayes, and KNN models using various subsets of the predictors. Describe your findings (Page 195)

```{r}
#Because I have MASS and ISLR2 loaded, both returned a 'Boston' data set. 
#The one in ISLR2 is obviously the one meant to be used here. 
Boston = ISLR2::Boston
medCrimRt = median(Boston$crim)
Boston$crim2 = ifelse(Boston$crim > medCrimRt, 1, 0)

bostonColors = c('red', 'black')[factor(Boston$crim2)]
pairs(Boston[, 1:ncol(Boston) - 1], col = bostonColors)
cors = round(cor(Boston[, 1:ncol(Boston) -1]), 2)
cors
```
```{r}
#For correlations greater than |0.40|, boxplots are shown next: 
par(mfrow = c(2, 3))
boxplot(indus ~ crim2, data = Boston, ylab = 'Prop Non-Retail Bus/town', xlab = 'Crime Rate vs. Median')
boxplot(nox ~ crim2, data = Boston, ylab = 'nitrogen oxide (pp10m)', xlab = 'Crime Rate vs. Median')
boxplot(rad ~ crim2, data = Boston, ylab = 'Accessibility to Rad Hwys', xlab = 'Crime Rate vs. Median')
boxplot(tax ~ crim2, data = Boston, ylab = 'tax-rate per $10k', xlab = 'Crime Rate vs. Median')
boxplot(lstat ~ crim2, data = Boston, ylab = '% Lower Stat Population', xlab = 'Crime Rate vs. Median')
```

```{r}
#A new dataframe is created for those variables relative to each other, 
#as well as to the response:
Boston2 = Boston[, c(3, 5, 9:10, 12, 14)]
pairs(Boston2, col = bostonColors)
```


```{r}
#Split Boston data set into train and test partitions
#I am choosing an 80/20 train/test split.

set.seed(123)
trainInd = sample(nrow(Boston2), 0.8 * nrow(Boston2), replace = FALSE)

bosTrain = Boston2[trainInd, ]
bosTest = Boston2[-trainInd, ]


#Logistic Regression Classifier

bosLRModel = glm(crim2 ~ indus + nox, data = bosTrain)
bosLRPreds = predict(bosLRModel, bosTest, type = 'response')
bosLRLabel = rep(0, nrow(bosTest))
bosLRLabel[bosLRPreds > 0.5] = 1
bosLRTable = table(bosLRLabel, bosTest$crim2)
bosLRTable
```
The error rate for the logistic regression using indus and nox variables was about 19%:

```{r}
1 - sum(diag(bosLRTable)) / sum(bosLRTable)
```

```{r}
#LDA Classifier

bosLDAModel = lda(crim2 ~ nox + rad, data = bosTrain)
bosLDAPreds = predict(bosLDAModel, bosTest, type = 'response')
bosLDATable = table(bosLDAPreds$class, bosTest$crim2)
bosLDATable

```
The error rate for the LDA Model using nox and rad variables was about 13%:

```{r}
1 - sum(diag(bosLDATable)) / sum(bosLDATable)
```

```{r}
#naive Bayes Classifier

bosNBModel = naiveBayes(crim2 ~ indus + nox + lstat, data = bosTrain)
bosNBPreds = predict(bosNBModel, bosTest)
bosNBTable = table(bosNBPreds, bosTest$crim2)
bosNBTable

```
The error rate for the naive Bayes Model using indus, nox, and lstat variables was about 20%:

```{r}
1 - sum(diag(bosNBTable)) / sum(bosNBTable)
```

```{r}
#KNN Classifier
#I am going to use all variables, and several values for K

bosKNNTrain = scale(bosTrain[, c(1:5)])
bosKNNTest = scale(bosTest[, c(1:5)])
bosKNNLabels = bosTrain$crim2

set.seed(123)

kVals = 20
bosKNNOutputs = data.frame(matrix(ncol = 2, nrow = kVals))
names(bosKNNOutputs) = c('K', 'Error')
knnTables = list()

for (knum in seq(1:kVals)) {
  bosKNNPreds = knn(bosKNNTrain, bosKNNTest, bosKNNLabels, k = knum)
  bosKNNTable = table(bosKNNPreds, bosTest$crim2)
  knnTables[[knum]] = bosKNNTable
  knnErrorRate = 1 - sum(diag(bosKNNTable)) / sum(bosKNNTable)
  knnOutputs$K[knum] = knum
  knnOutputs$Error[knum] = knnErrorRate
}

plot(knnOutputs$K, knnOutputs$Error, type = 'b', col = 'blue', 
     main = 'Error Rate for KNN Classifier by Value of K', 
     xlab = 'Value for K', 
     ylab = 'Test Error Rate', 
     ylim = c(0, max(knnOutputs$Error) + 0.05))

```
It looks like K = 4 produced the lowest test error rate. The actual value for this outcome was just under 6%: 

```{r}
knnTables[[4]]

1 - sum(diag(knnTables[[4]])) / sum(knnTables[[4]])
```




